{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2IMtqh6GYvoU"
      },
      "source": [
        "## Q-learning\n",
        "\n",
        "This notebook will guide you through implementation of vanilla Q-learning algorithm.\n",
        "\n",
        "You need to implement QLearningAgent (follow instructions for each method) and use it on a number of tests below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "colab_type": "code",
        "id": "492wo-yEYvoi",
        "outputId": "4c891050-13dd-4f95-c237-769655759397"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "!ls\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  sys.path.append('/content/drive/My Drive/RL/hw3')\n",
        "  %cd /content/drive/My Drive/RL/hw3\n",
        "\n",
        "  !pip install gym\n",
        "  !apt-get install -y xvfb\n",
        "  !wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb\n",
        "  !apt-get install -y python-opengl ffmpeg\n",
        "  !pip install pyglet==1.5.0\n",
        "\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# If you are running locally, just ignore it\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pWLVNEU5Yvot"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "        \"\"\"\n",
        "        Q-Learning Agent\n",
        "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
        "        Instance variables you have access to\n",
        "          - self.epsilon (exploration prob)\n",
        "          - self.alpha (learning rate)\n",
        "          - self.discount (discount rate aka gamma)\n",
        "\n",
        "        Functions you should use\n",
        "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
        "            which returns legal actions for a state\n",
        "          - self.get_qvalue(state,action)\n",
        "            which returns Q(state,action)\n",
        "          - self.set_qvalue(state,action,value)\n",
        "            which sets Q(state,action) := value\n",
        "        !!!Important!!!\n",
        "        Note: please avoid using self._qValues directly. \n",
        "            There's a special self.get_qvalue/set_qvalue for that.\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self, state, action, value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "    #---------------------START OF YOUR CODE---------------------#\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Compute your agent's estimate of V(s) using current q-values\n",
        "        V(s) = max_over_action Q(state,action) over possible actions.\n",
        "        Note: please take into account that q-values can be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        <YOUR CODE HERE >\n",
        "\n",
        "        return value\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "        \"\"\"\n",
        "\n",
        "        # agent parameters\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        <YOUR CODE HERE >\n",
        "\n",
        "        self.set_qvalue(state, action, < YOUR_QVALUE > )\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values). \n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        <YOUR CODE HERE >\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.  \n",
        "        With probability self.epsilon, we should take a random action.\n",
        "            otherwise - the best policy action (self.get_best_action).\n",
        "\n",
        "        Note: To pick randomly from a list, use random.choice(list). \n",
        "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
        "              and compare it with your probability\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        action = None\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # agent parameters:\n",
        "        epsilon = self.epsilon\n",
        "\n",
        "        <YOUR CODE HERE >\n",
        "\n",
        "        return chosen_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dN5c1ijJ3_bw"
      },
      "source": [
        "Now you are going to implement SARSA.\n",
        "\n",
        "<img src=https://miro.medium.com/max/3406/1*fYuXsaJoyCWuIZu49vx_GA.png width=800>\n",
        "<center><i>Source: Introduction to Reinforcement learning by Sutton and Barto —Chapter 6</i></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9AKYTeS_3-3b"
      },
      "outputs": [],
      "source": [
        "class SarsaAgent(QLearningAgent):\n",
        "    \"\"\" \n",
        "    An agent that changes some of q-learning functions to implement SARSA. \n",
        "    Note: this demo assumes that your implementation of QLearningAgent.update uses get_value(next_state).\n",
        "    If it doesn't, please add\n",
        "        def update(self, state, action, reward, next_state):\n",
        "            and implement it for SARSA's V(s')\n",
        "    \"\"\"\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\" \n",
        "        Returns Vpi for current state under epsilon-greedy policy:\n",
        "          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
        "\n",
        "        Hint: all other methods from QLearningAgent are still accessible.\n",
        "        \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Get value from (s,a,s',a') where a is chosen from Q with epsilon - greedy\n",
        "        <YOUR CODE HERE >\n",
        "\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EGMj7Y238iLl"
      },
      "source": [
        "Now you are going to implement Expected Value SARSA.\n",
        "\n",
        "<img src=https://miro.medium.com/max/2660/1*04P6VvjaGK2eiV0afZemPg.png width=800>\n",
        "<center><i>Source: Introduction to Reinforcement learning by Sutton and Barto —6.9</i></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Jp2uOGnc8hcF"
      },
      "outputs": [],
      "source": [
        "class EVSarsaAgent(QLearningAgent):\n",
        "    \"\"\" \n",
        "    An agent that changes some of q-learning functions to implement Expected Value SARSA. \n",
        "    Note: this demo assumes that your implementation of QLearningAgent.update uses get_value(next_state).\n",
        "    If it doesn't, please add\n",
        "        def update(self, state, action, reward, next_state):\n",
        "            and implement it for Expected Value SARSA's V(s')\n",
        "    \"\"\"\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\" \n",
        "        Returns Vpi for current state under epsilon-greedy policy:\n",
        "          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
        "\n",
        "        Hint: all other methods from QLearningAgent are still accessible.\n",
        "        \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        <YOUR CODE HERE >\n",
        "\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eT636X-RYvo0"
      },
      "source": [
        "### Try it on gridworld\n",
        "\n",
        "Here we use the qlearning agent on gridworld env from openai gym.\n",
        "The custom environment is taken from https://github.com/ibrahim-elshar/gym-windy-gridworlds. \n",
        "You will need to insert a few agent functions here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "colab_type": "code",
        "id": "f5QJYRCTYvo3",
        "outputId": "aaa68344-0d2e-4bf9-8b3a-2da8f60e2c64"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "#load custom environment\n",
        "import gym_windy_gridworlds\n",
        "\n",
        "env = gym.make(\"WindyGridWorld-v0\")\n",
        "n_actions = env.action_space.n\n",
        "env.reset()\n",
        "\n",
        "for i in range(1):\n",
        "    env.step(env.action_space.sample())\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "anPGspugYvpA"
      },
      "outputs": [],
      "source": [
        "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "                       get_legal_actions=lambda s: range(n_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XhH0YejZYvpK"
      },
      "outputs": [],
      "source": [
        "def play_and_train(env, agent, t_max=10**4):\n",
        "    \"\"\"\n",
        "    This function should \n",
        "    - run a full game, actions given by agent's e-greedy policy\n",
        "    - train agent using agent.update(...) whenever it is possible\n",
        "    - return total reward\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s.\n",
        "        a = <YOUR CODE >\n",
        "\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "\n",
        "        # train (update) agent for state s\n",
        "        <YOUR CODE HERE >\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "colab_type": "code",
        "id": "as94AYlGYvpR",
        "outputId": "0ef4c7f3-31f2-419c-de2d-365f3464c750"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "rewards = []\n",
        "for i in range(1000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "    agent.epsilon *= 0.99\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KeGp5cGH957Q"
      },
      "source": [
        "### Cliff World\n",
        "\n",
        "Let's now see how our algorithm compares against q-learning in case where we force agent to explore all the time.\n",
        "\n",
        "<img src=https://miro.medium.com/max/2072/1*FBJfEd_cuVW1CsDK7gCQsg.png width=300>\n",
        "<center><i>image by cs188</i></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UAmYlHMS95ml"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import gym.envs.toy_text\n",
        "env = gym.envs.toy_text.CliffWalkingEnv()\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(env.__doc__)\n",
        "# Agent can choose to go as close to the cliff as it wishes. x:start, T:exit, C:cliff, o: flat ground\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Hif7BC1L-G5Z"
      },
      "outputs": [],
      "source": [
        "def play_and_train(env, agent, t_max=10**4):\n",
        "    \"\"\"This function should \n",
        "    - run a full game, actions given by agent.getAction(s)\n",
        "    - train agent using agent.update(...) whenever possible\n",
        "    - return total reward\"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        agent.update(s, a, r, next_s)\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cZHiO4ny-XYs"
      },
      "outputs": [],
      "source": [
        "agent_sarsa = SarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                           get_legal_actions=lambda s: range(n_actions))\n",
        "\n",
        "agent_evsarsa = EVSarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                           get_legal_actions=lambda s: range(n_actions))\n",
        "\n",
        "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                          get_legal_actions=lambda s: range(n_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HyqcaA6K-ltk"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from pandas import DataFrame\n",
        "\n",
        "\n",
        "def moving_average(x, span=100): return DataFrame(\n",
        "    {'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "\n",
        "rewards_sarsa, rewards_ql = [], []\n",
        "\n",
        "for i in range(5000):\n",
        "    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n",
        "    rewards_ql.append(play_and_train(env, agent_ql))\n",
        "    # Note: agent.epsilon stays constant\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('EVSARSA mean reward =', np.mean(rewards_sarsa[-100:]))\n",
        "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
        "        plt.title(\"epsilon = %s\" % agent_ql.epsilon)\n",
        "        plt.plot(moving_average(rewards_sarsa), label='ev_sarsa')\n",
        "        plt.plot(moving_average(rewards_ql), label='qlearning')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.ylim(-500, 0)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LsF_Mxeh-tby"
      },
      "source": [
        "Let's now see what did the algorithms learn by visualizing their actions at every state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jtN9ObrI-uXh"
      },
      "outputs": [],
      "source": [
        "def draw_policy(env, agent):\n",
        "    \"\"\" Prints CliffWalkingEnv policy with arrows. Hard-coded. \"\"\"\n",
        "    n_rows, n_cols = env._cliff.shape\n",
        "\n",
        "    actions = '^>v<'\n",
        "\n",
        "    for yi in range(n_rows):\n",
        "        for xi in range(n_cols):\n",
        "            if env._cliff[yi, xi]:\n",
        "                print(\" C \", end='')\n",
        "            elif (yi * n_cols + xi) == env.start_state_index:\n",
        "                print(\" X \", end='')\n",
        "            elif (yi * n_cols + xi) == n_rows * n_cols - 1:\n",
        "                print(\" T \", end='')\n",
        "            else:\n",
        "                print(\" %s \" %\n",
        "                      actions[agent.get_best_action(yi * n_cols + xi)], end='')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qdjI5LAV-xhF"
      },
      "outputs": [],
      "source": [
        "print(\"Q-Learning\")\n",
        "draw_policy(env, agent_ql)\n",
        "\n",
        "print(\"SARSA\")\n",
        "draw_policy(env, agent_sarsa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1X9kmN8OYvpa"
      },
      "source": [
        "# Binarized state spaces\n",
        "\n",
        "Use agent to train efficiently on CartPole-v0.\n",
        "This environment has a continuous set of possible states, so you will have to group them into bins somehow.\n",
        "\n",
        "The simplest way is to use `round(x,n_digits)` (or numpy round) to round real number to a given amount of digits.\n",
        "\n",
        "The tricky part is to get the n_digits right for each state to train effectively.\n",
        "\n",
        "Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "colab_type": "code",
        "id": "XYN0zGILYvpc",
        "outputId": "c97b60a8-3eb7-49e0-d3cd-698023f1abb7"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "n_actions = env.action_space.n\n",
        "print(env.observation_space)\n",
        "print(\"first state:%s\" % (env.reset()))\n",
        "plt.imshow(env.render('rgb_array'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PdVGbyTiYvpg"
      },
      "source": [
        "### Play a few games\n",
        "\n",
        "We need to estimate observation distributions. To do so, we'll play a few games and record all states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "dpRkt9NiYvph",
        "outputId": "9e23774d-213d-4114-a6ba-5944c35a1c67"
      },
      "outputs": [],
      "source": [
        "all_states = []\n",
        "for _ in range(1000):\n",
        "    all_states.append(env.reset())\n",
        "    done = False\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(env.action_space.sample())\n",
        "        all_states.append(s)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "all_states = np.array(all_states)\n",
        "\n",
        "for obs_i in range(env.observation_space.shape[0]):\n",
        "    plt.hist(all_states[:, obs_i], bins=20)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c51mVHQ4Yvpm"
      },
      "source": [
        "## Binarize environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "545fRXiMYvpn"
      },
      "outputs": [],
      "source": [
        "from gym.core import ObservationWrapper\n",
        "\n",
        "\n",
        "class Binarizer(ObservationWrapper):\n",
        "\n",
        "    def observation(self, state):\n",
        "\n",
        "        # state = <round state to some amount digits.>\n",
        "        # hint: you can do that with round(x,n_digits)\n",
        "        # you will need to pick a different n_digits for each dimension\n",
        "        state[0] = np.round(state[0], 0)\n",
        "        state[1] = np.round(state[1], 1)\n",
        "        state[2] = np.round(state[2], 2)\n",
        "        state[3] = np.round(state[3], 1)\n",
        "\n",
        "        return tuple(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3UvSapETYvpr"
      },
      "outputs": [],
      "source": [
        "env = Binarizer(gym.make(\"CartPole-v0\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "sK7Y5Q3HYvpu",
        "outputId": "d462600a-bc4a-46dc-e27e-543e698a3e2f"
      },
      "outputs": [],
      "source": [
        "all_states = []\n",
        "for _ in range(1000):\n",
        "    all_states.append(env.reset())\n",
        "    done = False\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(env.action_space.sample())\n",
        "        all_states.append(s)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "all_states = np.array(all_states)\n",
        "\n",
        "for obs_i in range(env.observation_space.shape[0]):\n",
        "\n",
        "    plt.hist(all_states[:, obs_i], bins=20)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rxWBlN6EYvpx"
      },
      "source": [
        "## Learn binarized policy\n",
        "\n",
        "Now let's train a policy that uses binarized state space.\n",
        "\n",
        "__Tips:__ \n",
        "* If your binarization is too coarse, your agent may fail to find optimal policy. In that case, change binarization. \n",
        "* If your binarization is too fine-grained, your agent will take much longer than 1000 steps to converge. You can either increase number of iterations and decrease epsilon decay or change binarization.\n",
        "* Having 10^3 ~ 10^4 distinct states is recommended (`len(QLearningAgent._qvalues)`), but not required.\n",
        "* A reasonable agent should get to an average reward of >=50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "k5xacRfyYvpy"
      },
      "outputs": [],
      "source": [
        "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "                       get_legal_actions=lambda s: range(n_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "colab_type": "code",
        "id": "82hhSIx7Yvp0",
        "outputId": "313b9522-8921-4825-cb11-e04d83fcd4d4"
      },
      "outputs": [],
      "source": [
        "rewards = []\n",
        "agent.epsilon = 0.0001\n",
        "for i in range(1000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "\n",
        "    # OPTIONAL: you can adjust epsilon\n",
        "    agent.epsilon *= 0.99\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hw3_2_Q-learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
