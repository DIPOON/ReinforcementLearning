{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gym_Envs_1_preamble_evn_list.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prYhQX_dTzVC"
      },
      "source": [
        "> This notebook presents the preambles needed for working with Gym at Google CoLab and Gym Environments. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oow0rc2iaDZ4"
      },
      "source": [
        "# CoLab Preambles\n",
        "\n",
        "Most of the requirements of python packages are already fulfilled on CoLab. To run Gym, you have to install prerequisites like xvbf,opengl & other python-dev packages using the following codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wY4qZhPXotR"
      },
      "source": [
        "#!apt-get update --fix-missing\n",
        "!pip install -q gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt-get install -y xvfb x11-utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get atari game roms"
      ],
      "metadata": {
        "id": "X3Rzf9y6Q3_y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U99_zgNCk3t"
      },
      "source": [
        "!wget http://www.atarimania.com/roms/Roms.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x /content/Roms.rar"
      ],
      "metadata": {
        "id": "oDwtMpajMg54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/ROMS.zip"
      ],
      "metadata": {
        "id": "-ns8tTdWL8e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym-retro\n",
        "!python3 -m retro.import ROMS/"
      ],
      "metadata": {
        "id": "7dmkrXycHnz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giqCoXRwaaH3"
      },
      "source": [
        "For rendering environment, you can use pyvirtualdisplay. So fulfill that "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrxgO5S4XxI5"
      },
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUSaUTcgat3F"
      },
      "source": [
        "To activate virtual display we need to run a script once for training an agent, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn1IAnsDYK4V"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AutUqpSRYN1W"
      },
      "source": [
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh10T5veI1zk"
      },
      "source": [
        "import retro\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWNVK4NUJUCl"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTuJfCB_CYzh"
      },
      "source": [
        "# OpenAI Gym Available Environment\n",
        "\n",
        "Gym comes with a diverse suite of environments that range from easy to difficult and involve many different kinds of data. View the [full list of environments](https://gym.openai.com/envs) to get the birds-eye view.\n",
        "\n",
        "- [Classic control](https://gym.openai.com/envs#classic_control) and [toy text](https://gym.openai.com/envs#toy_text): complete small-scale tasks, mostly from the RL literature. They’re here to get you started.\n",
        "\n",
        "- [Algorithmic](https://gym.openai.com/envs#algorithmic): perform computations such as adding multi-digit numbers and reversing sequences. One might object that these tasks are easy for a computer. The challenge is to learn these algorithms purely from examples. These tasks have the nice property that it’s easy to vary the difficulty by varying the sequence length.\n",
        "\n",
        "- [Atari](https://gym.openai.com/envs#atari): play classic Atari games. \n",
        "\n",
        "- [2D and 3D robots](https://gym.openai.com/envs#mujoco): control a robot in simulation. These tasks use the MuJoCo physics engine, which was designed for fast and accurate robot simulation. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdouS9UAOymo"
      },
      "source": [
        "##  List the Environments Available in your Installation\n",
        "\n",
        "gym’s main purpose is to provide a large collection of environments that expose a common interface and are versioned to allow for comparisons. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H4RZv88Oy57"
      },
      "source": [
        "from gym import envs\n",
        "print(envs.registry.all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBphB64Qeoms"
      },
      "source": [
        "## Algorithmic\n",
        "\n",
        "These are a variety of algorithmic tasks, such as learning to copy a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZXrnpT_e4-h"
      },
      "source": [
        "env = gym.make('Copy-v0')\n",
        "env.reset()\n",
        "#plt.imshow(env.render())\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lJoXuxGeblC"
      },
      "source": [
        "## Atari\n",
        "\n",
        "The Atari environments are a variety of Atari video games. Gym is already installed but not with atari game environments, to get that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGn9Aj9sa4Vk"
      },
      "source": [
        "!pip install gym[atari]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEr-EChCcJtz"
      },
      "source": [
        "# Atari Environment\n",
        "env = retro.make('SpaceInvaders-Atari2600')\n",
        "height, width, channels = env.observation_space.shape\n",
        "actions = env.action_space.n\n",
        "\n",
        "episodes =5\n",
        "\n",
        "for episode in range(1, episodes):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  score = 0\n",
        "\n",
        "  while not done:\n",
        "    env.render(mode='rgb_array')\n",
        "    action = random.choice([0,1,2,3,4,5,6])\n",
        "    n_state, reward, done, info = env.step([action])\n",
        "    score += reward\n",
        "  print(f\"Episode : {episode} score : {score}\")\n",
        "  plt.imshow(env.render('rgb_array'))\n",
        "env.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsUglhhic_oE"
      },
      "source": [
        "## Box2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGuvFaoNcoBF"
      },
      "source": [
        "Box2d is a 2D physics engine. You can install it via  and then get started as follow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCSdSokZcztQ"
      },
      "source": [
        "!pip install gym[box2d]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmKsxcS7cjzZ"
      },
      "source": [
        "# Box2d Environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "env.reset()\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "#env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y9TBop7dF42"
      },
      "source": [
        "## Classic control\n",
        "These are a variety of classic control tasks, which would appear in a typical reinforcement learning textbook. If you didn't do the full install, you will need to run the following code to enable rendering. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYDe8axLdI1E"
      },
      "source": [
        "!pip install gym[classic_control]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q7p1iRwdGpE"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "#env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa_cmD3ydx6M"
      },
      "source": [
        "PyBullet Robotics Environments\n",
        "3D physics environments like the Mujoco environments but uses the Bullet physics engine and does not require a commercial license. Works on Mac/Linux/Windows.\n",
        "\n",
        "Learn more from Pybullet quick guide: https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.wz5to0x8kqmr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjMkS5U_kpBO"
      },
      "source": [
        "!pip install git+https://github.com/benelot/pybullet-gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHGmInyvlA6_"
      },
      "source": [
        "import pybulletgym  # register PyBullet enviroments with open ai gym\n",
        "\n",
        "#env = gym.make('HumanoidPyBulletEnv-v0')\n",
        "import pybullet_envs.bullet.minitaur_gym_env as e\n",
        "env = e.MinitaurBulletEnv()\n",
        "env.reset()\n",
        "plt.imshow(env.render('rgb_array'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrC7R_pleAzM"
      },
      "source": [
        "# I did not test the following two environments because of the MuJoCo License. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxxp8uFrde_t"
      },
      "source": [
        "## MuJoCo\n",
        "\n",
        "MuJoCo is a physics engine which can do very detailed efficient simulations with contacts. It's not open-source, so you'll have to follow the instructions in mujoco-py to set it up. Refer the following site\n",
        "https://gist.github.com/BuildingAtom/3119ac9c595324c8001a7454f23bf8c8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To run mujoco in google colab, run the following code\n",
        "import os\n",
        "if not os.path.exists('.mujoco_setup_complete'):\n",
        "  # Get the prereqs\n",
        "  !apt-get -qq update\n",
        "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "  # Get Mujoco\n",
        "  !mkdir ~/.mujoco\n",
        "  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
        "  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
        "  !rm mujoco.tar.gz\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n",
        "  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n",
        "  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
        "  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
        "  !ldconfig\n",
        "  # Install Mujoco-py\n",
        "  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
        "  # run once\n",
        "  !touch .mujoco_setup_complete\n",
        "\n",
        "try:\n",
        "  if _mujoco_run_once:\n",
        "    pass\n",
        "except NameError:\n",
        "  _mujoco_run_once = False\n",
        "if not _mujoco_run_once:\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  try:\n",
        "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
        "  except KeyError:\n",
        "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
        "  try:\n",
        "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  except KeyError:\n",
        "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  # presetup so we don't see output on first env initialization\n",
        "  import mujoco_py\n",
        "  _mujoco_run_once = True"
      ],
      "metadata": {
        "id": "t_kVtkRli1dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer here https://github.com/openai/mujoco-py"
      ],
      "metadata": {
        "id": "q4a_8q0Dqekc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQB9dhTidsy1"
      },
      "source": [
        "import mujoco_py\n",
        "import os\n",
        "mj_path = mujoco_py.utils.discover_mujoco()\n",
        "xml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\n",
        "model = mujoco_py.load_model_from_path(xml_path)\n",
        "sim = mujoco_py.MjSim(model)\n",
        "\n",
        "print(sim.data.qpos)\n",
        "\n",
        "sim.step()\n",
        "print(sim.data.qpos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_Ip8tMidvS0"
      },
      "source": [
        "env = gym.make('Humanoid-v2')\n",
        "env.reset()\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "#env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHBj7v7Wd0ML"
      },
      "source": [
        "env = gym.make('HandManipulateBlock-v0')\n",
        "env.reset()\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "#env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb-2ceEyo-El"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}