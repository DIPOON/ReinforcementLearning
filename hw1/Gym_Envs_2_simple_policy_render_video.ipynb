{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gym_Envs_2_simple_policy_render_video.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "prYhQX_dTzVC"
      },
      "cell_type": "markdown",
      "source": [
        "> This notebook presents shows how to solve some tasks with random action, assigned deterministic action, or heuristic action and how to render the process of tasks to video."
      ]
    },
    {
      "metadata": {
        "id": "Oow0rc2iaDZ4"
      },
      "cell_type": "markdown",
      "source": [
        "# CoLab Preambles\n",
        "\n",
        "Most of the requirements of python packages are already fulfilled on CoLab. To run Gym, you have to install prerequisites like xvbf,opengl & other python-dev packages using the following codes."
      ]
    },
    {
      "metadata": {
        "id": "7wY4qZhPXotR"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3U99_zgNCk3t"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym[atari]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "giqCoXRwaaH3"
      },
      "cell_type": "markdown",
      "source": [
        "For rendering environment, you can use pyvirtualdisplay. So fulfill that "
      ]
    },
    {
      "metadata": {
        "id": "yrxgO5S4XxI5"
      },
      "cell_type": "code",
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vUSaUTcgat3F"
      },
      "cell_type": "markdown",
      "source": [
        "To activate virtual display we need to run a script once for training an agent, as follows:"
      ]
    },
    {
      "metadata": {
        "id": "Pn1IAnsDYK4V"
      },
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AutUqpSRYN1W"
      },
      "cell_type": "code",
      "source": [
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jh10T5veI1zk"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JWNVK4NUJUCl"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3-ATQfrfYkM7"
      },
      "cell_type": "markdown",
      "source": [
        "# OpenAI Gym\n",
        "\n",
        "OpenAI gym is a python library that wraps many classical decision problems including robot control, videogames and board games. We will use the environments it provides to test our algorithms on interesting decision problems .\n",
        "\n",
        "This gym [wiki](https://github.com/openai/gym/wiki) explains more about the environment. \n",
        "\n",
        "Good general-purpose agents don't need to know the semantics of the observations: they can learn how to map observations to actions to maximize reward without any prior knowledge.\n",
        "\n",
        "That said, the meaning of Box(4,)\n",
        "[position of cart, velocity of cart, angle of pole, rotation rate of pole]. Defined at https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py#L75"
      ]
    },
    {
      "metadata": {
        "id": "5feQRBXTKqGI"
      },
      "cell_type": "markdown",
      "source": [
        "## Explore environment object\n",
        "\n",
        "- env.observation_space\n",
        "- env.action_space \n",
        "\n",
        "- env.reset() : reset environment to initial state, return first observation\n",
        "- env.render(): show current environment state (a more colorful version :) )\n",
        "- env.step(action) : commit action a and return (new observation, reward, is done, info)"
      ]
    },
    {
      "metadata": {
        "id": "RvFDvAsvH70n"
      },
      "cell_type": "markdown",
      "source": [
        "## CartPole"
      ]
    },
    {
      "metadata": {
        "id": "1Ea_5qNYIBsl"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('CartPole-v0')\n",
        "env = wrap_env(env) \n",
        "\n",
        "print('observation space:', env.observation_space)\n",
        "print('action space:', env.action_space)\n",
        "\n",
        "obs = env.reset()\n",
        "#env.render()\n",
        "\n",
        "print('initial observation:', obs)\n",
        "\n",
        "action = env.action_space.sample()  # take a random action\n",
        "\n",
        "obs, r, done, info = env.step(action)\n",
        "print('next observation:', obs)\n",
        "print('reward:', r)\n",
        "print('done:', done)\n",
        "print('info:', info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLeHP_dxLReW"
      },
      "cell_type": "markdown",
      "source": [
        "### Display Video"
      ]
    },
    {
      "metadata": {
        "id": "bhsj7BTPHepg"
      },
      "cell_type": "code",
      "source": [
        "'''CartPole problem use random action'''\n",
        "import gym\n",
        "env = gym.make('CartPole-v0')\n",
        "env = wrap_env(env)  # defined before for rendering online\n",
        "\n",
        "observation = env.reset()\n",
        "    \n",
        "while True:\n",
        "  env.render()\n",
        "  \n",
        "  # your agent goes here\n",
        "  action = env.action_space.sample()   # take a random action\n",
        "  observation, reward, done, info = env.step(action) \n",
        "  #print(reward)\n",
        "   \n",
        "  if done: \n",
        "    break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTN0hCKZIGdP"
      },
      "cell_type": "markdown",
      "source": [
        "## MountainCar"
      ]
    },
    {
      "metadata": {
        "id": "6uAbJY54Pjy4"
      },
      "cell_type": "markdown",
      "source": [
        "### Random Action"
      ]
    },
    {
      "metadata": {
        "id": "Vcj2F7fkYSUR"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "env = wrap_env(env)\n",
        "env.reset()\n",
        "#\n",
        "# explore MountainCar environment\n",
        "#\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)\n",
        "\n",
        "#\n",
        "# take random actions and show the video result\n",
        "# In MountainCar, observation is just two numbers: \n",
        "# car position and velocity\n",
        "#\n",
        "observation = env.reset()\n",
        "print(\"initial observation code:\", observation)\n",
        "\n",
        "    \n",
        "while True:\n",
        "  env.render()\n",
        "  \n",
        "  # your agent goes here\n",
        "  action = env.action_space.sample()   # take a random action\n",
        "  observation, reward, done, info = env.step(action) \n",
        "  # print(reward)\n",
        "   \n",
        "  if done: \n",
        "    break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-m03oAaPoRE"
      },
      "cell_type": "markdown",
      "source": [
        "### Assigned Action"
      ]
    },
    {
      "metadata": {
        "id": "cJJlwrykZRkL"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "env = wrap_env(env)\n",
        "env.reset()\n",
        "# explore MountainCar environment\n",
        "#\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0MOhVFFiQNxq"
      },
      "cell_type": "code",
      "source": [
        "# Explore a little bit\n",
        "obs = env.reset()\n",
        "print(\"initial observation code:\", obs)\n",
        "print(\"taking action 2 (right)\")\n",
        "new_obs, reward, is_done, _ = env.step(2)\n",
        "\n",
        "print(\"new observation code:\", new_obs)\n",
        "print(\"reward:\", reward)\n",
        "print(\"is game over?:\", is_done)\n",
        "\n",
        "# As you can see, the car has moved to the right slightly (around 0.0005)\n",
        "plt.imshow(env.render('rgb_array'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9rojutbnRgZV"
      },
      "cell_type": "markdown",
      "source": [
        "### MountainCar Video: Keep moving right by assignment"
      ]
    },
    {
      "metadata": {
        "id": "HyS4V0m9Rkvl"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "env = wrap_env(env)\n",
        "\n",
        "#\n",
        "# explore MountainCar environment\n",
        "#\n",
        "# plt.imshow(env.render('rgb_array'))\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)\n",
        "\n",
        "#\n",
        "# keep moving right and show the video result\n",
        "# In MountainCar, observation is just two numbers: \n",
        "# car position and velocity\n",
        "#\n",
        "observation = env.reset()\n",
        "print(\"initial observation code:\", observation)\n",
        "\n",
        "    \n",
        "#while True:\n",
        "for _ in range(1000):\n",
        "  env.render()\n",
        "  \n",
        "  # your agent goes here\n",
        "  print(\"taking action 2 (right)\")\n",
        "  action = 2   \n",
        "  observation, reward, done, info = env.step(action) \n",
        "  print(reward)\n",
        "   \n",
        "  if done: \n",
        "    env.reset()\n",
        "            \n",
        "env.close()\n",
        "show_video()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XQ_UXi_KZcJK"
      },
      "cell_type": "markdown",
      "source": [
        "### MountainCar with heuristic action strategy"
      ]
    },
    {
      "metadata": {
        "id": "z2btCBLfbiMc"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.envs.classic_control.mountain_car import MountainCarEnv"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ziwJH9-7ZvcQ"
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# create env manually to set time limit. Please don't change this.\n",
        "#\n",
        "TIME_LIMIT = 250\n",
        "env = gym.wrappers.TimeLimit(MountainCarEnv(), max_episode_steps=TIME_LIMIT + 1)\n",
        "env = wrap_env(env)\n",
        "\n",
        "obs = env.reset()\n",
        "actions = {'left': 0, 'stop': 1, 'right': 2}\n",
        "\n",
        "#\n",
        "# prepare \"display\"\n",
        "#\n",
        "#%matplotlib notebook\n",
        "#fig = plt.figure()\n",
        "#ax = fig.add_subplot(111)\n",
        "#fig.show()\n",
        "\n",
        "#\n",
        "# simple heuristic policy\n",
        "#\n",
        "def policy(t):\n",
        "    if t>50 and t<100:\n",
        "        return actions['left']\n",
        "    else:\n",
        "        return actions['right']\n",
        "\n",
        "\n",
        "for t in range(TIME_LIMIT):\n",
        "    \n",
        "    #\n",
        "    # change the line below to reach the flag\n",
        "    #\n",
        "    obs, r, done, _ = env.step(policy(t))\n",
        "    \n",
        "    # draw game image on display\n",
        "    #ax.clear()\n",
        "    #ax.imshow(env.render('rgb_array'))\n",
        "    #fig.canvas.draw()\n",
        "    \n",
        "    if done:\n",
        "        print(\"Well done!\")\n",
        "        break\n",
        "#    else:    \n",
        "#        print(\"Time limit exceeded. Try again.\")\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EeGFstZ7vvv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}